#!/usr/bin/env python3
"""
æ³•å¾‹ä»»åŠ¡å¥–åŠ±è®¡ç®—
Legal Task Reward Computation

å¤šç»´åº¦å¥–åŠ±:
- æ³•å¾‹ä¾æ®å‡†ç¡®æ€§ (35%)
- æ¨ç†é€»è¾‘è´¨é‡ (25%)
- ç½ªå/ç»“è®ºæ­£ç¡®æ€§ (20%)
- ç­”æ¡ˆå®Œæ•´æ€§ (20%)

5æ¡£ç»†ç²’åº¦å¥–åŠ±: [0.0, 0.2, 0.4, 0.7, 1.0]
"""

import re
from typing import Dict, List, Optional, Any, Tuple


class LegalRewardComputer:
    """æ³•å¾‹ä»»åŠ¡å¥–åŠ±è®¡ç®—å™¨"""

    # å¥–åŠ±æƒé‡
    WEIGHTS = {
        'legal_basis': 0.35,      # æ³•å¾‹ä¾æ®å‡†ç¡®æ€§
        'reasoning': 0.25,         # æ¨ç†é€»è¾‘è´¨é‡
        'conclusion': 0.20,        # ç»“è®ºæ­£ç¡®æ€§
        'completeness': 0.20       # ç­”æ¡ˆå®Œæ•´æ€§
    }

    # 5æ¡£å¥–åŠ±å€¼
    REWARD_LEVELS = [0.0, 0.2, 0.4, 0.7, 1.0]

    def __init__(self, llm=None):
        """
        Args:
            llm: LLMå®ä¾‹ç”¨äºLLM-as-Judgeè¯„ä¼°
        """
        self.llm = llm

    async def compute_reward(
        self,
        answer: str,
        ground_truth: Dict,
        task_type: str,
        jurisdiction: str = "CN",
        question: str = ""
    ) -> Tuple[float, Dict]:
        """
        è®¡ç®—æ³•å¾‹ä»»åŠ¡å¥–åŠ±

        Args:
            answer: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
            ground_truth: æ ‡å‡†ç­”æ¡ˆ/å‚è€ƒä¿¡æ¯
            task_type: ä»»åŠ¡ç±»å‹
            jurisdiction: ç®¡è¾–åŒº
            question: åŸå§‹é—®é¢˜

        Returns:
            (reward, details): å¥–åŠ±å€¼å’Œè¯¦ç»†è¯„åˆ†
        """
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        scores = {}

        # 1. æ³•å¾‹ä¾æ®å‡†ç¡®æ€§
        scores['legal_basis'] = await self._evaluate_legal_basis(
            answer, ground_truth, jurisdiction
        )

        # 2. æ¨ç†é€»è¾‘è´¨é‡
        scores['reasoning'] = await self._evaluate_reasoning(
            answer, question, jurisdiction
        )

        # 3. ç»“è®ºæ­£ç¡®æ€§
        scores['conclusion'] = self._evaluate_conclusion(
            answer, ground_truth, task_type, jurisdiction
        )

        # 4. ç­”æ¡ˆå®Œæ•´æ€§
        scores['completeness'] = self._evaluate_completeness(
            answer, task_type, jurisdiction
        )

        # åŠ æƒè®¡ç®—æ€»åˆ†
        total_score = sum(
            scores[dim] * self.WEIGHTS[dim]
            for dim in scores
        )

        # æ˜ å°„åˆ°5æ¡£å¥–åŠ±
        reward = self._map_to_reward_level(total_score)

        details = {
            'dimension_scores': scores,
            'weighted_total': total_score,
            'final_reward': reward,
            'jurisdiction': jurisdiction,
            'task_type': task_type
        }

        return reward, details

    async def _evaluate_legal_basis(
        self,
        answer: str,
        ground_truth: Dict,
        jurisdiction: str
    ) -> float:
        """
        è¯„ä¼°æ³•å¾‹ä¾æ®å‡†ç¡®æ€§

        æ£€æŸ¥:
        - CN: æ³•æ¡å¼•ç”¨æ˜¯å¦åŒ¹é… (ã€Šåˆ‘æ³•ã€‹ç¬¬264æ¡)
        - US: åˆ¤ä¾‹/æ³•è§„å¼•ç”¨æ˜¯å¦æ­£ç¡® (18 U.S.C. Â§ 1341)
        """
        gt_articles = ground_truth.get('articles', [])
        if not gt_articles:
            gt_articles = ground_truth.get('applicable_statutes', [])

        if not gt_articles:
            # æ— æ ‡å‡†ç­”æ¡ˆå‚è€ƒï¼Œä½¿ç”¨LLMè¯„ä¼°
            if self.llm:
                return await self._llm_evaluate_legal_basis(answer, jurisdiction)
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†

        # æå–ç­”æ¡ˆä¸­çš„æ³•å¾‹å¼•ç”¨
        cited_articles = self._extract_legal_citations(answer, jurisdiction)

        if not cited_articles:
            return 0.0  # æ— å¼•ç”¨

        # è®¡ç®—åŒ¹é…åº¦
        gt_set = set(str(a).lower() for a in gt_articles)
        cited_set = set(str(a).lower() for a in cited_articles)

        # Jaccardç›¸ä¼¼åº¦
        intersection = len(gt_set & cited_set)
        union = len(gt_set | cited_set)

        if union == 0:
            return 0.0

        similarity = intersection / union

        # ç²¾ç¡®åŒ¹é…åŠ åˆ†
        exact_matches = len([c for c in cited_articles if any(
            str(g).lower() in str(c).lower() or str(c).lower() in str(g).lower()
            for g in gt_articles
        )])

        bonus = min(0.2, exact_matches * 0.1)

        return min(1.0, similarity + bonus)

    async def _llm_evaluate_legal_basis(self, answer: str, jurisdiction: str) -> float:
        """ä½¿ç”¨LLMè¯„ä¼°æ³•å¾‹ä¾æ®"""
        if not self.llm:
            return 0.5

        if jurisdiction == "CN":
            prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ³•å¾‹å›ç­”ä¸­çš„æ³•å¾‹ä¾æ®å¼•ç”¨è´¨é‡ï¼ˆ0-10åˆ†ï¼‰ï¼š

{answer}

è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†ï¼šæ³•æ¡å¼•ç”¨å®Œå…¨æ­£ç¡®ï¼Œæ ¼å¼è§„èŒƒï¼ˆã€Šæ³•å¾‹åã€‹ç¬¬Xæ¡ï¼‰
- 7åˆ†ï¼šå¼•ç”¨åŸºæœ¬æ­£ç¡®ï¼Œæœ‰å°é”™è¯¯
- 4åˆ†ï¼šå¼•ç”¨ä¸å®Œæ•´æˆ–éƒ¨åˆ†é”™è¯¯
- 0åˆ†ï¼šæ— å¼•ç”¨æˆ–å®Œå…¨é”™è¯¯

åªè¾“å‡ºåˆ†æ•°ï¼ˆ0-10çš„æ•´æ•°ï¼‰ï¼š"""
        else:
            prompt = f"""Rate the legal citation quality in this answer (0-10):

{answer}

Criteria:
- 10: Perfect citations in Bluebook format
- 7: Mostly correct with minor errors
- 4: Incomplete or partially incorrect
- 0: No citations or completely wrong

Output only the score (integer 0-10):"""

        try:
            response = await self.llm.aask(msg=prompt)
            score = int(re.search(r'\d+', response).group())
            return min(1.0, score / 10.0)
        except:
            return 0.5

    async def _evaluate_reasoning(
        self,
        answer: str,
        question: str,
        jurisdiction: str
    ) -> float:
        """
        è¯„ä¼°æ¨ç†é€»è¾‘è´¨é‡

        ä½¿ç”¨LLM-as-Judgeæˆ–åŸºäºè§„åˆ™è¯„ä¼°
        """
        # åŸºæœ¬æ£€æŸ¥
        length_score = min(1.0, len(answer) / 500)  # è‡³å°‘500å­—ç¬¦

        # ç»“æ„æ£€æŸ¥
        structure_score = 0.0
        if jurisdiction == "CN":
            structure_markers = ['é¦–å…ˆ', 'å…¶æ¬¡', 'å› æ­¤', 'ç»¼ä¸Š', 'æ ¹æ®', 'ä¾æ®', 'æœ¬æ¡ˆ']
        else:
            structure_markers = ['first', 'second', 'therefore', 'accordingly', 'pursuant', 'holding']

        found_markers = sum(1 for m in structure_markers if m.lower() in answer.lower())
        structure_score = min(1.0, found_markers / 3)

        # LLMè¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.llm:
            llm_score = await self._llm_evaluate_reasoning(answer, question, jurisdiction)
            return 0.3 * length_score + 0.2 * structure_score + 0.5 * llm_score

        return 0.5 * length_score + 0.5 * structure_score

    async def _llm_evaluate_reasoning(self, answer: str, question: str, jurisdiction: str) -> float:
        """ä½¿ç”¨LLMè¯„ä¼°æ¨ç†è´¨é‡"""
        if jurisdiction == "CN":
            prompt = f"""è¯„ä¼°ä»¥ä¸‹æ³•å¾‹æ¨ç†çš„é€»è¾‘è´¨é‡ï¼ˆ0-10åˆ†ï¼‰ï¼š

é—®é¢˜ï¼š{question[:200]}

å›ç­”ï¼š{answer[:1000]}

è¯„åˆ†æ ‡å‡†ï¼š
- é€»è¾‘è¿è´¯æ€§
- è®ºè¯å……åˆ†æ€§
- æ³•å¾‹æ¨ç†è§„èŒƒæ€§

åªè¾“å‡ºåˆ†æ•°ï¼š"""
        else:
            prompt = f"""Rate the legal reasoning quality (0-10):

Question: {question[:200]}

Answer: {answer[:1000]}

Criteria:
- Logical coherence
- Argument sufficiency
- Legal reasoning standards

Output only the score:"""

        try:
            response = await self.llm.aask(msg=prompt)
            score = int(re.search(r'\d+', response).group())
            return min(1.0, score / 10.0)
        except:
            return 0.5

    def _evaluate_conclusion(
        self,
        answer: str,
        ground_truth: Dict,
        task_type: str,
        jurisdiction: str
    ) -> float:
        """
        è¯„ä¼°ç»“è®ºæ­£ç¡®æ€§

        æ ¹æ®ä»»åŠ¡ç±»å‹ä½¿ç”¨ä¸åŒè¯„ä¼°æ–¹æ³•
        """
        if task_type == 'case_prediction':
            return self._evaluate_case_prediction(answer, ground_truth, jurisdiction)
        elif task_type == 'statute_qa':
            return self._evaluate_statute_qa(answer, ground_truth)
        elif task_type == 'document_gen':
            return self._evaluate_document_gen(answer, ground_truth)
        else:  # consultation
            return self._evaluate_consultation(answer, ground_truth)

    def _evaluate_case_prediction(
        self,
        answer: str,
        ground_truth: Dict,
        jurisdiction: str
    ) -> float:
        """
        è¯„ä¼°æ¡ˆä»¶é¢„æµ‹ç»“æœ

        CN: ç½ªåã€æ³•æ¡ã€åˆ‘æœŸåŒ¹é…
        US: HoldingåŒ¹é…
        """
        score = 0.0

        if jurisdiction == "CN":
            # ç½ªååŒ¹é…
            gt_charges = ground_truth.get('charges', [])
            if gt_charges:
                charge_match = any(
                    charge in answer for charge in gt_charges
                )
                if charge_match:
                    score += 0.5

            # æ³•æ¡åŒ¹é…
            gt_articles = ground_truth.get('articles', [])
            if gt_articles:
                article_match = any(
                    str(art) in answer for art in gt_articles
                )
                if article_match:
                    score += 0.3

            # é‡åˆ‘åŒ¹é…ï¼ˆå¦‚æœ‰ï¼‰
            sentence = ground_truth.get('sentence', {})
            if sentence:
                # ç®€åŒ–ï¼šåªæ£€æŸ¥æ˜¯å¦æåŠåˆ‘æœŸ
                if 'å¹´' in answer or 'æœˆ' in answer or 'æœ‰æœŸå¾’åˆ‘' in answer:
                    score += 0.2

        else:  # US
            # HoldingåŒ¹é…
            gt_holding = ground_truth.get('correct_holding', '')
            gt_answer = ground_truth.get('answer', '')

            if gt_holding and gt_holding.lower() in answer.lower():
                score = 1.0
            elif gt_answer and gt_answer.lower() in answer.lower():
                score = 0.8
            else:
                # éƒ¨åˆ†åŒ¹é…
                gt_text = gt_holding or gt_answer
                if gt_text:
                    gt_words = set(gt_text.lower().split())
                    answer_words = set(answer.lower().split())
                    overlap = len(gt_words & answer_words) / max(len(gt_words), 1)
                    score = overlap * 0.6

        return min(1.0, score)

    def _evaluate_statute_qa(self, answer: str, ground_truth: Dict) -> float:
        """è¯„ä¼°æ³•æ¡é—®ç­”ç»“æœ"""
        gt_answer = ground_truth.get('answer', '')

        if not gt_answer:
            return 0.5

        # ç®€å•æ–‡æœ¬åŒ¹é…
        gt_lower = gt_answer.lower()
        answer_lower = answer.lower()

        if gt_lower in answer_lower:
            return 1.0

        # å…³é”®è¯åŒ¹é…
        gt_words = set(gt_lower.split())
        answer_words = set(answer_lower.split())
        overlap = len(gt_words & answer_words) / max(len(gt_words), 1)

        return min(1.0, overlap)

    def _evaluate_document_gen(self, answer: str, ground_truth: Dict) -> float:
        """è¯„ä¼°æ³•å¾‹æ–‡ä¹¦ç”Ÿæˆç»“æœ"""
        # æ£€æŸ¥æ–‡ä¹¦åŸºæœ¬è¦ç´ 
        required_elements = [
            'åŸå‘Š', 'è¢«å‘Š', 'è¯‰è®¼è¯·æ±‚', 'äº‹å®ä¸ç†ç”±',  # CN
            'plaintiff', 'defendant', 'prayer', 'facts'  # US
        ]

        found = sum(1 for e in required_elements if e.lower() in answer.lower())
        return min(1.0, found / 4)

    def _evaluate_consultation(self, answer: str, ground_truth: Dict) -> float:
        """è¯„ä¼°æ³•å¾‹å’¨è¯¢ç»“æœ"""
        gt_answer = ground_truth.get('answer', '')

        if not gt_answer:
            # æ— æ ‡å‡†ç­”æ¡ˆï¼Œæ£€æŸ¥ç­”æ¡ˆè´¨é‡
            if len(answer) > 200:
                return 0.6
            elif len(answer) > 100:
                return 0.4
            else:
                return 0.2

        # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        gt_words = set(gt_answer.lower().split())
        answer_words = set(answer.lower().split())
        overlap = len(gt_words & answer_words) / max(len(gt_words), 1)

        return min(1.0, overlap * 1.2)

    def _evaluate_completeness(
        self,
        answer: str,
        task_type: str,
        jurisdiction: str
    ) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆå®Œæ•´æ€§

        æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ³•å¾‹è¦ç´ 
        """
        completeness_checklist = {
            'case_prediction': {
                'CN': ['ç½ªå', 'æ³•æ¡', 'é‡åˆ‘', 'ç†ç”±'],
                'US': ['charge', 'statute', 'holding', 'reasoning']
            },
            'statute_qa': {
                'CN': ['æ³•æ¡', 'è§£é‡Š', 'é€‚ç”¨'],
                'US': ['statute', 'interpretation', 'application']
            },
            'document_gen': {
                'CN': ['åŸå‘Š', 'è¢«å‘Š', 'è¯·æ±‚', 'ç†ç”±', 'è¯æ®'],
                'US': ['plaintiff', 'defendant', 'relief', 'facts', 'evidence']
            },
            'consultation': {
                'CN': ['å»ºè®®', 'ä¾æ®', 'é£é™©'],
                'US': ['advice', 'authority', 'risk']
            }
        }

        checklist = completeness_checklist.get(task_type, {}).get(jurisdiction, [])

        if not checklist:
            return 0.5

        found = sum(1 for item in checklist if item.lower() in answer.lower())
        return found / len(checklist)

    def _extract_legal_citations(self, text: str, jurisdiction: str) -> List[str]:
        """æå–æ³•å¾‹å¼•ç”¨"""
        citations = []

        if jurisdiction == "CN":
            # åŒ¹é…ã€Šxxxã€‹ç¬¬xxxæ¡
            pattern = r'ã€Š[^ã€‹]+ã€‹[ç¬¬]?\d+[æ¡æ¬¾]?'
            citations = re.findall(pattern, text)

            # åŒ¹é… åˆ‘æ³•ç¬¬xxxæ¡
            pattern2 = r'[åˆ‘æ°‘è¡Œè¯‰][æ³•å…¸][ç¬¬]\d+[æ¡æ¬¾]?'
            citations.extend(re.findall(pattern2, text))

        else:  # US
            # åŒ¹é… X U.S.C. Â§ XXXX
            pattern = r'\d+\s*U\.S\.C\.\s*Â§\s*\d+'
            citations = re.findall(pattern, text)

            # åŒ¹é…æ¡ˆä¾‹å¼•ç”¨ XXX v. XXX
            pattern2 = r'[A-Z][a-z]+\s+v\.\s+[A-Z][a-z]+'
            citations.extend(re.findall(pattern2, text))

        return citations

    def _map_to_reward_level(self, score: float) -> float:
        """å°†åˆ†æ•°æ˜ å°„åˆ°5æ¡£å¥–åŠ±"""
        if score >= 0.9:
            return 1.0
        elif score >= 0.7:
            return 0.7
        elif score >= 0.5:
            return 0.4
        elif score >= 0.3:
            return 0.2
        else:
            return 0.0


def test_reward_computer():
    """æµ‹è¯•å¥–åŠ±è®¡ç®—å™¨"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•æ³•å¾‹å¥–åŠ±è®¡ç®—å™¨")
    print("="*60)

    computer = LegalRewardComputer()

    # æµ‹è¯•CNæ¡ˆä¾‹
    cn_answer = """æ ¹æ®æ¡ˆæƒ…åˆ†æï¼Œè¢«å‘Šäººå¼ æŸçš„è¡Œä¸ºæ„æˆç›—çªƒç½ªã€‚
    ä¾æ®ã€Šåˆ‘æ³•ã€‹ç¬¬264æ¡è§„å®šï¼Œç›—çªƒå…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚
    æœ¬æ¡ˆä¸­ï¼Œå¼ æŸç›—çªƒè´¢ç‰©ä»·å€¼5000å…ƒï¼Œå±äºæ•°é¢è¾ƒå¤§ï¼Œå»ºè®®åˆ¤å¤„æœ‰æœŸå¾’åˆ‘ä¸€å¹´ã€‚"""

    cn_gt = {
        'charges': ['ç›—çªƒç½ª'],
        'articles': ['åˆ‘æ³•ç¬¬264æ¡'],
        'sentence': {'imprisonment_months': 12}
    }

    # åŒæ­¥æµ‹è¯•å„ç»´åº¦
    legal_basis = computer._extract_legal_citations(cn_answer, "CN")
    print(f"\nğŸ“‹ æå–çš„æ³•å¾‹å¼•ç”¨: {legal_basis}")

    completeness = computer._evaluate_completeness(cn_answer, 'case_prediction', 'CN')
    print(f"ğŸ“Š å®Œæ•´æ€§è¯„åˆ†: {completeness:.2f}")

    conclusion = computer._evaluate_case_prediction(cn_answer, cn_gt, 'CN')
    print(f"ğŸ“Š ç»“è®ºè¯„åˆ†: {conclusion:.2f}")


if __name__ == "__main__":
    test_reward_computer()
